\section{Proposed Evaluation}

In a full experimental evaluation, we will undertake to answer the following core research questions:

\begin{itemize}
  \item {\bf RQ1}: Does replacing time spent fuzzing a target program with time spent fuzzing mutants of the target program improve
  the effectiveness of fuzzing?
  \item {\bf RQ2:} Does using prioritization improve the effectiveness of fuzzing with mutants?  If so, which prioritizations perform best?
  \item {\bf RQ3:} How do non-cumulative (parallel) and cumulative (sequential) mutant fuzzing compare?
  \item {\bf RQ4:} For non-cumulative mutant fuzzing, is improving the corpus by first fuzzing the target program when it has changed worthwhile?
  \end{itemize}
  
{\bf RQ1} is the overall question of whether any variant of fuzzing using mutants increases standard fuzzing evaluation metrics
(unique faults detected and code coverage, primarily).  {\bf RQ2}-{\bf RQ4} consider some of the primary choices to be made in implementing fuzzing mutants.

The experiments will be based on widely-used benchmarks, and conform to the standards proposed by Klees et. al \cite{evalfuzz}, e.g., using 10 or
more runs of 24 hours each in experimental trials.  We will make every effort to identify and protect against the usual threats to validity in fuzzing experiments, by using a range of benchmark subjects and avoiding pitfalls such as measuring only crash counts bucketed crashes, rather than making an effort to identify actual distinct faults \cite{FuzzAppeal} (or using only crashes, not crashes and code coverage results).

Our plan is to make use of Google's FuzzBench \cite{metzman2021fuzzbench} (\url{https://google.github.io/fuzzbench/}) to perform primary experiments.  The lead of the FuzzBench team has extended an invitation to do so, and started discussing the details of compling and selecting mutants with us.  Using FuzzBench will allow us to perform extensive experiments in a well-curated configuration, over a number of important fuzzers and benchmarks.  Ideally, we can compare performance with and without (variants of) our approach on all of these fuzzers.  Use of FuzzBench will limit the potential for experimental error or poor experimental design choices, and make future comparison with other fuzzing techniques easier.

We would like to also evaluate our approach against T-Fuzz \cite{tfuzz}, but the last commit to the repository (\url{https://github.com/HexHive/T-Fuzz}) was in December of 2018, and our early efforts to get the tool to build and operate in an environment comparable to what will be used for other fuzzers has not been promising (the versions of angr and other required tools are ancient and do not work with recent versions of Linux).  We will continue to explore this possibility, and potentially run an independent smaller comparison outside the FuzzBench context, if we can get T-Fuzz to work using the T-Fuzz docker image.

In addition to the primary research questions above, we plan to
examine other practically important aspects of mutant fuzzing.  For
instance, while we expect most gains to be derived from using corpus
inputs to help fuzz the target itself, we also believe, based on our
preliminary experiments discussed below, that some bugs may be found
only by fuzzing a mutant.  How often does this happen on real
programs, and why does it happen?  One possibility of interest is that the coarse heuristics many fuzzers use to avoid storing duplicate crashes \cite{semantic-crash-bucketing,mdebug} may sometimes discard non-redundant bugs, and that program mutants interact with AFL's heuristics to prevent this in some cases.  We also plan to identify particular mutants that contributed to hitting hard-to-reach program paths, in order to better understand if there are patterns in useful mutants that can be predicted.
