\section{Evaluation Method}

In our experimental evaluation, we undertook to answer the following core research questions:

\begin{itemize}
  \item {\bf RQ1}: Can replacing time spent fuzzing a target program with time spent fuzzing mutants of the target program, under at least one configuration, improve
  the effectiveness of fuzzing on average over a variety of target programs?
  \item {\bf RQ2:} Does using prioritization improve the effectiveness of fuzzing with mutants?  If so, which prioritizations perform best?
  \item {\bf RQ3:} How do non-cumulative (parallel) and cumulative (sequential) mutant fuzzing compare?
  \item {\bf RQ4:} How does impact of using mutants vary with the fraction of the fuzzing budget devoted to fuzzing mutants, given a good configuration based on results from {\bf RQ2} and {\bf RQ3}?
  \item {\bf RQ5:} Are certain kinds of mutants generally more useful for fuzzing, or generally not useful?  Are these patterns the same or different for new path discovery and novel bug detection?
  \end{itemize}
  
{\bf RQ1} is the overall question of whether \emph{any} variant of fuzzing using mutants explored increases standard fuzzing evaluation metrics
(unique faults detected and code coverage, primarily).  {\bf RQ2}-{\bf RQ5} consider some of the primary choices to be made in implementing fuzzing mutants, and help investigate why mutants are helpful, supposing an affirmative answer to {\bf RQ1}.

{\bf RQ2} and {\bf RQ3} and are straightforward, aiming to compare the effectivness of different basic approaches to using mutants in fuzzing (random or systematic selection of mutants, cumulative and non-cumulative approaches, respectively).  Because exploring all combinations of mutant-fuzzing approach and budget would require a prohibitive number of experimental runs, we planned to focus the investigation of {\bf RQ4} on answering the question, given a best-practice for using mutants in a baseline one-half-budget setting.  The core idea of {\bf RQ4} is to determine if, for most programs, one half of the budget is fuzzing mutants too long (such that diminishing returns have set in before this point, usually) or if further fuzzing of mutants, up to essentially the full budget, would continue to be useful, allowing for bypassing more difficult barriers to exploration.

Finally, for {\bf RQ5}, while the universalmutator does not label mutations with operators, it is possible to label generated mutant source files with the regular expression rule that produced the mutant, and thus categorize mutants broadly, though not using exactly the standard operator definitions.  Given such a categorization, we can determine if inputs produced by certain kinds of mutants tend to produce more new input paths that are valid for the original program, or produce more bugs only detected during mutant fuzzing.  The second question is particularly interesting: while we expect most gains to be derived from using corpus
inputs to help fuzz the target itself, we observe based on our
preliminary experiments discussed below, that some bugs may be found
\emph{only} by fuzzing a mutant.  How often does this happen on real
programs, and why does it happen?  One possibility of interest is that the coarse heuristics many fuzzers use to avoid storing duplicate crashes \cite{semantic-crash-bucketing,mdebug} may sometimes discard non-redundant bugs, and that program mutants interact with AFL's heuristics to prevent this in some cases.  {\bf RQ5} is more exploratory than the other research questions, in part aiming to discover which of the kinds of patterns proposed in the introduction appear most in practice.  However, results for {\bf RQ5} might also have more pragmatic importance, in informing refinement of mutant prioritization.

The set of experimental configurations shown below in the preliminary experiments gives an idea of the basic parameters to be varied.  The set of fuzzers included in the real experiment will be larger and there will also be an exploration of different choices for the portion of the time budget devoted to fuzzing mutants.  The experiments will be based on widely-used benchmarks, and conform to the standards proposed by Klees et. al \cite{evalfuzz}, e.g., using 10 or
more runs of 24 hours each in experimental trials.  We will make every effort to identify and protect against the usual threats to validity in fuzzing experiments, by using a range of benchmark subjects and avoiding pitfalls such as measuring only crash counts bucketed crashes, rather than making an effort to identify actual distinct faults \cite{FuzzAppeal} (or using only crashes, not crashes and code coverage results).

We  used Google's FuzzBench \cite{metzman2021fuzzbench} (\url{https://google.github.io/fuzzbench/}) to perform experiments.  The lead of the FuzzBench team extended an invitation to use FuzzBench and assisted us in our efforts.  Using FuzzBench allowed us to perform extensive experiments in a well-curated configuration, over a number of important fuzzers and benchmarks.  Use of FuzzBench limited the potential for experimental error or poor experimental design choices, and should make future comparison with other fuzzing techniques easier.

We would also like to have evaluated our approach against T-Fuzz \cite{tfuzz}, but the last commit to the repository (\url{https://github.com/HexHive/T-Fuzz}) was in December of 2018, and our early efforts to get the tool to build and operate in an environment comparable to what will be used for other fuzzers was not promising (the versions of angr and other required tools are ancient and do not work with recent versions of Linux).  Section \ref{sec:tfuzzfail} discusses our failure to produce useful T-Fuzz results.
